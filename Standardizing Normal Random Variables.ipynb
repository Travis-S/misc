{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Problem Statement#\n",
    "Given a random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, how is it that the random variable $Z = \\frac{X-\\mu}{\\sigma}$ is $\\mathcal{N}(0, 1)$? It is this question we are going to answer.\n",
    "\n",
    "**Intuitive Justification**\n",
    "\n",
    "Notice that if $\\langle X \\rangle = \\mu$, then $\\langle Z \\rangle = \\frac{\\langle X \\rangle - \\mu}{\\sigma^{2}} = 0$. Further, we have $\\langle Z^{2} \\rangle = \\frac{1}{\\sigma^{2}}\\langle (X-\\mu)^{2}\\rangle = \\frac{\\langle X^{2} \\rangle - 2 \\mu \\langle X \\rangle + \\mu^{2}}{\\sigma^{2}} = \\frac{\\langle X^{2} - \\mu^{2}}{\\sigma^{2}} = \\frac{Var(X)}{\\sigma^{2}} = 1$.\n",
    "\n",
    "However, just because the expected value and variance of $Z$ match the standard normal distribution does not necessarily mean the _distribution_ of $Z$ is $\\mathcal{N}(0,1)$. So while the above provides some justification, we have more work to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The Main Technique - Characteristic Functions#\n",
    "\n",
    "To demonstrate the result, we will rely on the characteristic function. Given a probability measure $\\mu(x)$, the characteristic function $\\psi(t)$ is defined as\n",
    "\n",
    "$$\\psi(t) = \\langle e^{itX}\\rangle = \\int e^{itx}~d\\mu(x)$$\n",
    "\n",
    "Assuming $\\mu(x) = p(x)~dx$ for some probability density function (pdf) $p$, we have\n",
    "\n",
    "$$\\psi_{X}(t) = \\int p(x)e^{itx}~dx$$\n",
    "\n",
    "You may recognize this as the _Fourier transform_ of the pdf. Hence, we have little to be worried about, since Fourier transforms occur in many diverse branches of mathematics, and are well-studied. Note: We use the subscript $_{X}$ to denote which random variable the characteristic function is representing.\n",
    "\n",
    "\n",
    "**Joint pdfs $p(x,y)$**\n",
    "\n",
    "Eventually, we will need to consider joint pdfs $p(x,y)$. Let's show a useful result about characteristic functions when the random variables $X$ and $Y$ are independent and we are considering quanties of the form $X + Y$.\n",
    "\n",
    "By definition, the characteristic function of $\\psi(t)$ is given as \n",
    "\n",
    "$$\\psi_{X+Y}(t) = \\int p(x, y)e^{it(x+y)}~dx~dy$$\n",
    "\n",
    "Now, under the assumption $X$ and $Y$ are independent, then $p(x, y) = a(x)b(y)$ for some pdfs $a$ and $b$. Plugging this in above, we see\n",
    "\n",
    "$$\\boxed{\\psi_{X+Y}(t) = \\int a(x)e^{itx}~dx \\int b(y)e^{ity}~dy = \\psi_{X}(t)\\psi_{Y}(t)}$$\n",
    "\n",
    "Thus, if $X$ and $Y$ are independent random variables, then the characteristic function of their sum is simply the product of their characterstic functions.\n",
    "\n",
    "**Rescaling**\n",
    "\n",
    "Another result we will need is to consider random variables of the form $X/c$ for some constant $c$. Plugging this into the definition gives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
